{"cells":[{"cell_type":"markdown","source":["###Syed Sharjeelullah\nFebruary 28th, 2022"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c6a0735-6bba-4b9b-9cb2-92d09300cb81"}}},{"cell_type":"markdown","source":["#####Please note that I have made a few assumptions here based on my understanding on the question. I have created the staging and production database environments on my local Azure cloud account, and have dropped the 2 csv files in an Azure Storage blob container in my personal Azure Storage Account.\n\nAssumptions:\n1. Staging and production environments are in Azure cloud.(I can provide the credentials for testing this code)\n2. The csv files were dropped in an Azure Storage Blob. (credentials to which will be provided)\n3. The final data views are stored in tables 'unique_providers' and 'unique_clinicians' in the production database in Azure.\n4. It is assumed that the input file format would be exactly the same for this ETL process.\n5. There are obvious improvements in this pipeline, but the focus was to implement a feasible ETL pipeline.\n6. Please fill out line 16-31 with the accurate credentials supplied to you in an encrypted file.\n7. Hardcoding credentials is a bad practice, and this was only done for demonstration purposes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc3e510e-970c-4950-96f5-3273c02c3c77"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import row_number,lit\nfrom pyspark.sql.window import Window\nimport datetime, time\nfrom pyspark.sql.types import IntegerType,BooleanType,DateType,StringType\nfrom pyspark.sql.functions import lit,unix_timestamp,to_date,to_timestamp,from_unixtime\nfrom pyspark.sql.functions import substring_index\n\n#SparkSession Initialization\nsession = SparkSession.builder.getOrCreate()\n\n#Connection variables for the staging environment which is a Postgresql database hosted in the Azure environment\n#Please note for the purposes of this excercise the credentials are hardcoded here as these databases are hosted in my personal azure account\n#However, it is recommended to never hardcode credentials as a general practice, and encryption would be required for HIPAA complaince\ndriver = \"org.postgresql.Driver\"\nurl = \"\"\ntable_c = \"public.clinicians\"\ntable_p = \"public.providers\"\nuser = \"\"\npassword = \"\"\n\n#Connection variables for the production environment which is a Postgresql database hosted in the Azure environment\ndriver_p = \"org.postgresql.Driver\"\nurl_p = \"\"\ntable_c1 = \"prod.clinicians_mart\"\ntable_p1 = \"prod.providers_mart\"\ntable_c2 = \"prod.unique_clinicians\"\ntable_p2 = \"prod.unique_providers\"\nuser_p = \"\"\npassword_p = \"\"\n\ndef fetch_data(session):\n  try:\n    #Establishing connection to Azure storage blob\n    session.conf.set(\n    \"fs.azure.sas.files.blob.core.windows.net\",\n    \"sp=r&st=2022-02-26T23:31:30Z&se=2022-02-27T07:31:30Z&spr=https&sv=2020-08-04&sr=c&sig=CiblBjHXmj7dJizB69IMWhRaoWhAoGBMljo8XFDpe6o%3D\"\n  )\n    #Reading files into dataframes\n    providers = session.read.csv(\n    \"wasb://files@clinicianfiles.blob.core.windows.net/providers.csv\", header = True\n  )\n    clinician_data = session.read.csv(\n    \"wasb://files@clinicianfiles.blob.core.windows.net/clinician data.csv\", header = True\n  )\n    flag = True\n  except Exception as e:\n    print(\"Error connecting to Azure storage or in reading files from Azure Storage\")\n    \n  return providers, clinician_data, flag\n\ndef staging_data(providers, clinician_data):\n  \n  #Timestamp to be stored to track the rows inserted\n  ct = datetime.datetime.now()\n  timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n  \n  #Preparing clinician dataframe to be stored into the staging envrionment\n  #Creating and adding columns like 'row_inserted_at', 'row_updated-at' and 'status' to track when the rows were inserted or updated and their status of whether they processed by the ETL pipeline or not\n  clinician_data = clinician_data.select(\"ID\", \"provider first\", 'provider last','email address','Sex','Care Center Name','primary language', 'NPI', 'title', 'updated_date')\n  clinician_data = clinician_data.withColumnRenamed('ID', 'id').withColumnRenamed(\"provider first\", 'first_name').withColumnRenamed(\"provider last\", 'last_name').withColumnRenamed('email address','email').withColumnRenamed('Care Center Name','care_center').withColumnRenamed('Sex','gender').withColumnRenamed('primary language','languages')\n  clinician_data = clinician_data.withColumn('row_inserted_at',unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\")).withColumn('row_updated_at',unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\")).withColumn(\"status\", lit('Ready for Processing')).withColumn(\"id\", clinician_data.id.cast('int'))\n  \n  #Preparing providers dataframe to be stored into the staging envrionment\n  #Creating and adding columns like 'row_inserted_at', 'row_updated-at' and 'status' to track when the rows were inserted or updated and their status of whether they processed by the ETL pipeline or not\n  providers = providers.select(\"id\", \"first name\", 'last_name','email','gender','Care Center','languages', 'NPI', 'title')\n  providers = providers.withColumnRenamed('first name', 'first_name').withColumnRenamed('Care Center','care_center')\n  providers = providers.withColumn(\"row_inserted_at\", unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\")).withColumn(\"row_updated_at\", unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\")).withColumn(\"status\", lit('Ready for Processing')).withColumn(\"id\", providers.id.cast('int'))\n  \n  return providers, clinician_data\n\ndef load_to_staging(providers, clinicians_data):\n\n  try:\n    #Writing out dataframes to the staging environment which contains already created tables for these two schemas\n    clinician_data.select('id', 'first_name', 'last_name','email','gender','care_center','languages', 'NPI', 'title', 'updated_date','row_inserted_at','row_updated_at', 'status').write.format(\"jdbc\")\\\n    .option(\"url\", url) \\\n    .option(\"driver\", driver).option(\"dbtable\", table_c) \\\n    .option(\"user\", user).option(\"password\", password) \\\n    .mode(\"append\") \\\n    .save()\n    \n    providers.select('id', 'first_name', 'last_name','email','gender','care_center','languages', 'NPI', 'title','row_inserted_at','row_updated_at', 'status').write.format(\"jdbc\")\\\n    .option(\"url\", url) \\\n    .option(\"driver\", driver).option(\"dbtable\", table_p) \\\n    .option(\"user\", user).option(\"password\", password) \\\n    .mode(\"append\") \\\n    .save()\n    \n    return_code = 'SUCCESS'\n    \n  except Exception as e:\n      output = f\"{e}\"\n      return_code = 'FAIL'\n      \n  return return_code\n\n#Reading from the staging database and only fetching unprocessed rows, to be prepared to be transformed and loaded into the data mart\ndef read_for_processing():\n  \n  try:\n    provider_df = spark.read.format(\"jdbc\").option(\"url\", url) \\\n    .option(\"driver\", driver).option(\"dbtable\", table_p) \\\n    .option(\"user\", user).option(\"password\", password).load()\n    \n    provider_df.createOrReplaceTempView('providers')\n    provider_df = spark.sql(\"SELECT * FROM providers WHERE status = 'Ready for Processing'\")\n\n    #Reading from the staging database and only fetching rows that were not processed before\n    clinician_df = spark.read.format(\"jdbc\").option(\"url\", url) \\\n    .option(\"driver\", driver).option(\"dbtable\", table_c) \\\n    .option(\"user\", user).option(\"password\", password).load()\n\n    clinician_df.createOrReplaceTempView('clinicians')\n    clinician_df = spark.sql(\"SELECT * FROM clinicians WHERE status = 'Ready for Processing'\")\n    return_bool = True\n  except Exception as e:\n    output = f\"{e}\"\n    return_code = 'FAILED TO READ DATA FROM STAGING'\n    print(return_code)\n    return_bool = False\n     \n  return provider_df, clinician_df, return_bool\n\ndef prep_for_datamart(provider_df, clinician_df):  \n  \n  ct = datetime.datetime.now()\n  timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')  \n  \n  #Provider data being prepped to be loaded into datamart\n  #Only including NPI digits after the hyphen '-'\n  #Only including Unique NPI's\n  cleaned_provider_df = provider_df.withColumn(\"NPI\", substring_index(provider_df[\"NPI\"], '-', -1).alias('right')) #right of delim\n  cleaned_provider_df = cleaned_provider_df.dropDuplicates([\"NPI\"])\n  cleaned_provider_df = cleaned_provider_df.withColumn('row_inserted_at',unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\")).withColumn('row_updated_at',unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n  cleaned_provider_df = cleaned_provider_df.select('first_name','last_name','email','gender','care_center','languages','NPI','title','row_inserted_at','row_updated_at','provider_staging_id')  \n  \n  #Clinician data being prepped to be loaded into datamart\n  #Only including NPI digits after the hyphen '-'\n  #Only including Unique NPI's\n  cleaned_clinician_df = clinician_df.withColumn(\"NPI\", substring_index(clinician_df[\"NPI\"], '-', -1).alias('right')) #right of delim\n  cleaned_clinician_df = cleaned_clinician_df.dropDuplicates([\"NPI\"])\n  cleaned_clinician_df = cleaned_clinician_df.withColumn('row_inserted_at',unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\")).withColumn('row_updated_at',unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n  cleaned_clinician_df = cleaned_clinician_df.select('first_name','last_name','email','gender','care_center','languages','NPI','title','updated_date','row_inserted_at','row_updated_at','clinician_staging_id')\n  \n  try:\n    cleaned_provider_df.select('first_name','last_name','email','gender','care_center','languages','NPI','title','row_inserted_at','row_updated_at','provider_staging_id')\\\n    .write.format(\"jdbc\")\\\n          .option(\"url\", url_p) \\\n          .option(\"driver\", driver_p).option(\"dbtable\", table_p1) \\\n          .option(\"user\", user_p).option(\"password\", password_p) \\\n          .mode(\"append\") \\\n          .save()\n    \n    cleaned_clinician_df.select('first_name','last_name','email','gender','care_center','languages','NPI','title','updated_date','row_inserted_at',\n                                'row_updated_at','clinician_staging_id').write.format(\"jdbc\")\\\n    .option(\"url\", url_p) \\\n    .option(\"driver\", driver_p).option(\"dbtable\", table_c1) \\\n    .option(\"user\", user_p).option(\"password\", password_p) \\\n    .mode(\"append\") \\\n    .save()\n    return_code = 'LOADED DATA IN THE MART SUCCESSFULLY'\n  except Exception as e:\n    output = f\"{e}\"\n    return_code = 'FAILED TO READ DATA FROM STAGING'\n  \n  return return_code\n\n#This function would store the transformed dataframes into tables 'unique-provider' and 'unique-clinicians', with unique NPI's and titles as the data points as given in the assignment\ndef storing_transformed_views():\n  \n  ct = datetime.datetime.now()\n  timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')  \n  \n  #Reading from the production datamart\n  try:\n    final_providers_df = spark.read.format(\"jdbc\").option(\"url\", url_p) \\\n    .option(\"driver\", driver_p).option(\"dbtable\", table_p1) \\\n    .option(\"user\", user_p).option(\"password\", password_p).load()\n\n    final_clinician_df = spark.read.format(\"jdbc\").option(\"url\", url_p) \\\n      .option(\"driver\", driver_p).option(\"dbtable\", table_c1) \\\n      .option(\"user\", user_p).option(\"password\", password_p).load()\n      \n    bool1 = True\n  except Exception as e:\n    msg = 'FAILED TO READ DATA FROM PRODUCTION'\n    print(msg)\n  \n  try:\n    #Now creating a new dataframe with field ['provider_internal_id', 'NPI', 'title', 'created_at']\n    #This would be stored in tables unique_clinicians and unique_providers\n    final_providers_df.createOrReplaceTempView('providermart')\n    final_providers_df = spark.sql(\"SELECT provider_internal_id, NPI, title FROM providermart\")\n    final_providers_df = final_providers_df.withColumn('created_at', unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n\n    final_providers_df.select('*').write.format(\"jdbc\")\\\n        .option(\"url\", url_p) \\\n        .option(\"driver\", driver_p).option(\"dbtable\", table_p2) \\\n        .option(\"user\", user_p).option(\"password\", password_p) \\\n        .mode(\"append\") \\\n        .save()\n\n    final_clinician_df.createOrReplaceTempView('clinicianmart')\n    final_clinician_df = spark.sql(\"SELECT clinician_internal_id, NPI, title FROM clinicianmart\")\n    final_clinician_df = final_clinician_df.withColumn('created_at', unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n\n    final_clinician_df.select('*').write.format(\"jdbc\")\\\n        .option(\"url\", url_p) \\\n        .option(\"driver\", driver_p).option(\"dbtable\", table_c2) \\\n        .option(\"user\", user_p).option(\"password\", password_p) \\\n        .mode(\"append\") \\\n        .save()\n    bool2 = True\n  except:\n    msg = \"Writing to production tables 'unique_providers' and/or 'unique_clinicians' failed\"\n    print(msg)\n    \n  if bool1 and bool2 == True:\n    msg = \"Complete Success\"\n    \n  return msg\n\n#The main function or program handler  \ndef main():\n  \n  start = time.time()\n  log = ''\n  \n  try:\n    providers, clinician_data, flag = fetch_data(session)\n    print(\"Step 1 Successful\")\n    log = log + \"Step 1 Successful\" + \", \"\n  except:\n    print(\"Step 1 Failed\")\n    log = log + \"Step 1 Failed\" + \", \"\n\n  try:  \n    if flag == True:\n      provider, clinician_data = staging_data(providers,clinician_data)\n      print(\"Step 2 Successful\")\n      log = log + \"Step 2 Successful\" + \", \"\n  except:\n    print(\"Step 2 Failed\")\n    log = log + \"Step 2 Failed\" + \", \"\n  \n  try:\n    if load_to_staging(provider, clinician_data) == 'SUCCESS':\n      provider_df, clinician_df, readbool = read_for_processing()\n      print(\"Step 3 Successful\") \n      log = log + \"Step 3 Successful\" + \", \"\n    else:\n      print(\"Load to Staging Failed\")\n      log = log + \"Load to Staging Failed\" + \", \"\n  except:\n    print(\"Step 3 Failed\")  \n    log = log + \"Step 3 Failed\" + \", \"\n    \n  try:\n    if prep_for_datamart(provider_df, clinician_df) == 'LOADED DATA IN THE MART SUCCESSFULLY':\n      print(\"Step 4 Successful\")\n      log = log + \"Step 4 Successful\" + \", \"\n      step4 = True\n  except:\n    print(\"Step 4 Failed\")\n    log = log + \"Step 4 Failed\" + \", \"\n    step4 = False\n    \n  if step4 == True:\n    if storing_transformed_views() == \"Complete Success\":\n        print(\"Step 5 Successful\")\n        print(\"The ETL pipeline ran successfully\")\n        log = log + \"Step 5 Successful\"\n  else:\n    print(\"Step 5 Failed\")\n    log = log + \"Step 5 Failed\"\n\n  print('Runtime: ', time.time() - start, ' Seconds')\n  \n  return log\n\nif __name__ == \"__main__\": \n  print(main())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"PySpark Program (Question 2)","showTitle":true,"inputWidgets":{},"nuid":"e1130877-b97b-4573-923c-c9d96fad4d16"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Step 1 Successful\nStep 2 Successful\nStep 3 Successful\nStep 4 Successful\nStep 5 Successful\nThe ETL pipeline ran successfully\nRuntime:  19.819820165634155  Seconds\nStep 1 Successful, Step 2 Successful, Step 3 Successful, Step 4 Successful, Step 5 Successful\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Step 1 Successful\nStep 2 Successful\nStep 3 Successful\nStep 4 Successful\nStep 5 Successful\nThe ETL pipeline ran successfully\nRuntime:  19.819820165634155  Seconds\nStep 1 Successful, Step 2 Successful, Step 3 Successful, Step 4 Successful, Step 5 Successful\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pytest\n  \ndef test_main():\n  logs = main()\n  assert 'Failed' not in logs, \"Test Failed, one or more of the steps Failed in main()\"\n  assert logs is not None, \"Test Failed, logs did not populate\"\n  print(\"Test Successful: main test passed\")\n  \ndef test_Azure_connectivity():\n  providers, clinicians, flag = fetch_data(session)\n  assert flag == True, \"Test failed: could not load data from Azure Storage\"\n  print(\"Test Successful: Azure Storage Connectivity Successful\")\n  \ndef test_storing_transformed_views():\n  assert storing_transformed_views() == \"Complete Success\", \"Test Failed: transfored views were not stored back in the prod db\"\n  print(\"Test Successful: test_storing_transformed_views Successful\")\n  \ndef test_prep_for_datamart():\n  providers, clinicians, flag = read_for_processing()\n  if flag == True:\n    assert prep_for_datamart(providers, clinicians) == 'LOADED DATA IN THE MART SUCCESSFULLY', \"Test Failed: dataframes were not stored successfuly in the mart tables\"\n    print(\"Test Successful: test_prep_for_datamart successful\")\n  \ndef test_read_for_processing():\n    p,c, readflag = read_for_processing()\n    assert readflag == True, \"Test Failed: Was not able to successfully retrieve unprocessed rows from staging\"\n    print(\"Test Successful: Unprocessed rows retreived successfully from staging\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Test Cases for the PySpark Program","showTitle":true,"inputWidgets":{},"nuid":"01710309-84a8-497a-bd37-e7a9e26bcb07"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#test_Azure_connectivity()\n# test_main()\n# test_storing_transformed_views()\n# test_read_for_processing()\n#test_prep_for_datamart()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Test out test case functions","showTitle":true,"inputWidgets":{},"nuid":"18e09129-9016-4a5e-8388-f7615fee18c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Test Successful: test_prep_for_datamart successful\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Test Successful: test_prep_for_datamart successful\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"LifeStance-Health-Question2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4411313731657113}},"nbformat":4,"nbformat_minor":0}
